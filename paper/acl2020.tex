%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{ {images/} }
\raggedbottom
\usepackage{colortbl}
\usepackage{color}
\usepackage{booktabs}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Evaluating the Efficacy of Attention Mechanisms on the Captioning of Images Taken by Visually Impaired People}

\author{Catherine Viloria \\
  University of Gothenburg \\ 
  \texttt{gusviloca@student.gu.se}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
yaaa
\end{abstract}

\section{Introduction}

Image caption generation continues to be a pertinent task within both the computer vision community and the natural language processing (NLP) research field. However, it is important for researchers to also consider how advancements in this field can and will affect users in the real world. Developing models that will be able to effectively and efficiently assist visually impaired users in their day-to-day lives is a challenge in the field to this day. Models that are considered to have a high performance in automatically generating captions will not necessarily perform well in a domain-specific task. Captions for the visually impaired must be detailed yet concise and prioritise the objects of interest within the image. Another factor that could affect a model’s performance would be the quality of the images that are typically associated with caption generation for this task. It is not uncommon for images to be in the wrong orientation, unfocused, and out of frame. While any improvement in image captioning is still a step forward, critically examining applications of these models in situations that are not as contrived is extremely important. 

In this project, the VizWiz dataset presented by \citet{Gurari-2020-captioning}, specifically created to represent the blind and visually impaired community, is used to train an encoder-decoder model with attention to examine how a standard image captioning model will perform in generating captions for the visually impaired. Evaluating the neural image caption generator with visual attention presented by \citet{Xu-2015-show-attend} through a domain-specific lens is important for the community being affected but also will have research-related contributions. Choosing to assess a model in this way presents nuances that are sometimes overlooked when implementing these models. It will also shed light on how humans would approach a task and eventually the possibility of replicating said modus operandi or intuition in a model. The problem of the gap between the two modalities—image and text—presented in \citet{Bernardi-2016-automatic} is also discussed. The extent of how bad this problem will be in this model is shown to be exacerbated because of the dataset used since grounding becomes an even more difficult task. 

The following sections are organised as follows. Section 2 introduces image caption generation and the models and attention mechanism that precede what was implemented for this paper. In Section 3, the VizWiz dataset and specifics of the model implementation are given. The results are presented in Section 4. Section 5 discusses the results. Specific plans for future work are defined in Section 6 and it is followed by the conclusion and future work in Section 7. References are provided in Section 8.

\section{Related Work}
Automatic description generation from images has been a challenging task that both the computer vision and natural language processing communities have taken an interest in. \citet{Bernardi-2016-automatic} surveys the current approaches to this task and classifies these approaches through how they conceptualise the problem—either a generation problem, a retrieval from a visual space problem, or a retrieval from a multimodal space problem. All three approaches have their advantages and disadvantages. The first two approaches choose to rely more heavily on one modality (visual or textual) to base the predictions for generating captions while the third approach attempts to combine both representations to make the final image description. The implementation in this paper is the third approach. This approach attempts to address the semantic gap or mapping problem between language and image representations through the attentional component. The attention mechanism allows us to see visualizations of which regions in an image are salient and where the model is “looking” when generating each word in the caption.

The model implemented in this paper is a combination of proposed methods from two different, but related, research areas—image captioning and machine translation. \citet{Vinyals-2014-showandtell}’s seminal paper \emph{Show and Tell} presents a neural image caption generator (NIC) that uses a vision convolutional neural network (CNN) that encodes images with richer representations to a fixed dimension for the decoder. The encoder’s output is then passed to a language-generating recurrent neural network (RNN)—an LSTM-based sentence generator. Their model demonstrates that this approach significantly improves both accuracy and fluency when generating captions. A neural machine translation with an attention mechanism is proposed in \citet{Bahdanau-2014-neural}. Attention allows the model to look for parts in the source sentence that would be most relevant in predicting the target word. The flexibility of alignment between the source and target sentences introduced through attention allows for better translations, especially between languages that have syntactical differences. 

The combination of NIC and an attention mechanism resulted in the model implemented by \citet{Xu-2015-show-attend}—a neural image caption generator with visual attention. The encoder uses low-level representations which prioritises salient features of an image. The attention component adds “an extra layer of interpretability to the output of the model” \citep{Xu-2015-show-attend} and the alignments learned by the model correspond very well with human intuition. The motivations and intuitions of choosing to use the model implemented by \citet{Xu-2015-show-attend} for this domain-specific task is discussed in Section 3. 


\section{Dataset \& Methodology}
\subsection{VizWiz Dataset}
The domain-specific dataset used for this project was the VizWiz dataset \footnote{\url{https://vizwiz.org/}}. It is comprised of 39,181 images taken by people who are either blind or visually impaired. The images of the dataset come from a mobile phone application made to support real-world users through their visual description service. Users take a picture, record a spoken question and are then connected with people that could assist them. The VizWiz dataset has also been developed for various computer vision tasks, all related and extremely useful for the visually impaired community: image captioning, image quality assessment, visual question answering (VQA), answer-difference reasoning, vision skills for VQA, and visual privacy recognition. The VizWiz dataset is “a valuable foundation for designing more generalized computer vision algorithms that meet the large diversity of needs for real end users” \citep{Gurari-2020-captioning}. 

\subsubsection{VizWiz-Captions Dataset}
For this project, the VizWiz-Captions dataset \footnote{\url{https://vizwiz.org/tasks-and-datasets/image-captioning/}} was used. Each image is paired with 5 human-made captions collected through crowdsourcing via Amazon Mechanical Turk. Due to GPU limitations and consciousness of memory usage, only the training dataset was used. The training dataset originally had 23,141 images and 117,155 captions. After examining some of the metadata included in the dataset, a filtering process was conducted. If an image was considered to be spam (\emph{is\_rejected} column) or considered to have quality issues by annotators (\emph{is\_precanned} column, if True had “Quality issues are too severe to recognize visual content.” as its caption), they were filtered out of the dataset. Images with the characteristic of having text were kept since text is one of the main reasons why someone who is visually impaired would need assistance. After filtering, there were 14,790 images left and the model implementation was done with a 60/20/20 train, validation, test split (8,874 train, 2,958 validation, and 2,958 test).

\subsubsection{Annotator Reliability \& Ethical Considerations}
Although it is incredibly important to collect human-made captions for any text generation task, the process and its output also present some challenges and factors that must be considered. While it is greatly advantageous for a model to learn from human-made captions, humans are not perfect and also make mistakes. Crowdsourcing allows researchers to reach a larger, more widespread and diverse demographic but it also makes it much more difficult to oversee and guide annotators to ensure that the captions being made are of the best quality. The instructions provided to annotators presented in \citet{Gurari-2020-captioning}, while understandably so given the conditions of the annotation process, were very concise and left space for restrictions on the annotators that would affect the dataset and future model implementations.  

Examples of instructions that could lead to problems when training models are: “Include at least eight words.”, “Describe all parts of the image that may be important to a person who is blind.”, and “If text is in the image, and is important, then you can summarize what it says. DO NOT use all the specific phrases that you see in the image as your description of the image.” \citep{Gurari-2020-captioning} If an annotator was able to effectively describe an image with 4 words (e.g. “Can of mushroom soup.”), they are then required to add additional information that might not be neither useful nor necessary. The second instruction can also be interpreted as subjective and what one annotator thinks is useful information for a person who is blind, another annotator might not think similarly. Finally, the third instruction requiring a summarization of text that appears in an image rather than providing a word-for-word description makes it extremely difficult to ground any generated caption for a model. Although text summarization would be useful for this real-world task, it is a completely different task that is overlooked when training an image caption generator. 

Since annotators were also found through crowdsourcing, annotator reliability could possibly be a point of contention. Not only are the annotators’ backgrounds unknown, they are also inconsistent. In an ideal world, it would have been better to have real-world users with high ratings of the mobile application to come in and annotate images as they have domain-specific knowledge and skills. Ethical concerns may also affect the quality of work annotators submit. It is extremely important to acknowledge the absolutely dismal wages provided to annotators through Amazon Mechanical Turk.\citep{hara2018amazon} The low wages and absence of work benefits can also affect the care and quality of the captions made by annotators. With these considerations, an example is provided in \autoref{fig:vwcaption} of how varied and inconsistent the quality and structure of captions can be within the dataset. The variation in complexity and length is quite large and can be argued to be either advantageous or disadvantageous for a model when training. 

\begin{figure}[h]
  \includegraphics[width=\linewidth]{VizWiz_train_00011794_captioned.png}
  \caption{An image from the VizWiz-Captions dataset and 5 captions from 5 different annotators.}
  \label{fig:vwcaption}
\end{figure}

\subsection{Model Architecture}
The model implemented was an encoder-decoder with an attention mechanism. The PyTorch Tutorial to Image Captioning by Sagar Vinodababu\footnote{\url{https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#training}} was followed and the implementation included an encoder that was pretrained on ResNet. The encoder is responsible for taking the images as input and transforming it into smaller representations. It returns a “state” that is fed into the decoder. The decoder is then responsible for interpreting the output of the encoder. It also calculates the attention weights for the image. (*also generates caption/ language model??)

Creating low-level representations when passed through the encoder seemed like a good approach for this task since a lot of images in the dataset only have one object as the focal point and this object is only partially shown in the image. A lower-level representation or a representation that is closer to what the image actually looks like would be very helpful rather than having a representation that is an average of everything in the picture. This approach would be called the top-down approach—contrasted with the bottom-up approach which is good for semantically-rich images where multiple objects within the image are first identified and then given to the encoder. \citep{Anderson-2017-bottomuptopdown} Datasets that have high-quality and semantically-rich images, such as MSCOCO, are more ideal for the top-down approach.

During training, sequences are padded then ordered by length in order to minimise computations (****???wording). Word embeddings, attention weights, and the decoder all have a dimension of 512 with a batch size of 32. The encoder’s learning rate was 0.0001 and the decoder’s learning rate was 0.0004. Early stopping was implemented after 20 epochs of no improvement. These hyperparameters and implementations were all chosen through the tutorial source.  

The greedy search algorithm was implemented for generating captions. In the greedy search algorithm, when given a number of choices in the next stage, the model will always choose the best immediate output. For this task, the language model takes the vocabulary’s logits and turns them into probabilities. This is then fed to the greedy search algorithm where it chooses the highest probable word to occur next at each stage. However, greedy search also has its drawbacks. Since it only chooses the best immediate output and does not look farther ahead, it has a higher chance of not reaching the globally optimum solution—hence its name, \emph{greedy} search.

\section{Results}
\label{sec:results}

Upon initial review, results were abysmal but with further examination and analysis, lessons were learned. In this section, results will be presented and their causes will also be explained. The sections following will include the extrapolations of the results and the conclusions that come with them. 

The corpus-BLEU score was used as the main measure for this project which requires a list of references (the human-made annotations) and a candidate (the predicted caption). It is the average of the scores between the candidate and each reference. Both sentence BLEU and corpus BLEU calculate the cumulative 4-gram BLEU score (BLEU-4) by default. Although a great option to calculate scores for a task that has more than one caption, longer sentences also lead to lower scores. Since the variation of lengths, even with a minimum of 8 words per caption, are quite large, this task will still be affected as seen on \autoref{tab:bctest}. Detailed captions are also valued for this task. However, the detail provided by annotators are often distinctive to the individual as annotators were encouraged to summarise texts and decide what in the image they think might be important for a blind person.

%---table: bc test---
\begin{table}[h]
\centering
\begin{tabular}{llll} 
\toprule
 & \textbf{Loss} & \textbf{Accuracy} & \textbf{BLEU-4} \\ 
\hline
\textit{BC} & 4.114 & 64.060 & 0.142178 \\
TEST & 4.181 & 63.941 & 0.145776 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\label{tab:bctest}Loss, Top-5 Accuracy, and BLEU-4 scores for the best performing model (BC for best checkpoint) during training and testing.}
\end{table}
%--- end of table

The training process also included a mechanism that saves and labels the best performing model as a checkpoint which will be used for validation and testing. \autoref{fig:lossgraph} and \autoref{fig:accuracygraph} show the loss and accuracy curves during training. The blue circle marker indicates the model used for validation and testing. In both curves, there are clearly other epochs where the model’s loss was lower, and the accuracy was higher. However, the model chosen to proceed with was decided through the highest BLEU-4 score. This can also be seen in \autoref{trainingdata} where the highest accuracy was attained on epoch 12 while the highest BLEU-4 score was attained on epoch 14. 

% loss graph figure
\begin{figure}[h]
  \includegraphics[width=\linewidth]{paper/images/loss_graph.png}
  \caption{Loss curve during training. The blue point is the loss for the best performing model model based on its BLEU-4 score.}
  \label{fig:lossgraph}
\end{figure}
% ----end of figure----

% accuracy graph figure
\begin{figure}[h]
  \includegraphics[width=\linewidth]{paper/images/accuracy_graph.png}
  \caption{Accuracy curve during training. The blue point is the accuracy for the best performing model based on its BLEU-4 score.}
  \label{fig:accuracygraph}
\end{figure}
% ----end of figure----

% training stats table
\begin{table}
\centering
\begin{tabular}{|c|c|c|} 
\hline
\rowcolor[rgb]{0.761,0.761,0.761} \textbf{Epochs} & \textbf{Top-5 Accuracy} & \textbf{BLEU-4}  \\ 
\hline
0                                                 & 55.982                  & 0.1047           \\ 
\hline
1                                                 & 59.216                  & 0.1180           \\ 
\hline
2                                                 & 60.894                  & 0.1277           \\ 
\hline
3                                                 & 62.122                  & 0.1307           \\ 
\hline
4                                                 & 62.798                  & 0.1362           \\ 
\hline
5                                                 & 63.192                  & 0.1371           \\ 
\hline
6                                                 & 63.632                  & 0.1404           \\ 
\hline
7                                                 & 63.87                   & 0.1429           \\ 
\hline
8                                                 & 64.06                   & 0.1421           \\ 
\hline
9                                                 & 64.045                  & 0.1440           \\ 
\hline
10                                                & 64.1                    & 0.1447           \\ 
\hline
11                                                & 64.112                  & 0.1449           \\ 
\hline
12                                                & \textbf{64.193}         & 0.1451           \\ 
\hline
13                                                & 64.031                  & 0.1445           \\ 
\hline
\rowcolor[rgb]{0.992,0.992,0.588} 14              & 63.923                  & \textbf{0.1465}  \\ 
\hline
15                                                & 63.912                  & 0.1461           \\ 
\hline
16                                                & 63.808                  & 0.1460           \\ 
\hline
17                                                & 63.753                  & 0.1459           \\ 
\hline
18                                                & 63.526                  & 0.1456           \\ 
\hline
19                                                & 63.594                  & 0.1452           \\ 
\hline
20                                                & 63.283                  & 0.1427           \\ 
\hline
21                                                & 63.275                  & 0.1429           \\ 
\hline
22                                                & 62.96                   & 0.1434           \\ 
\hline
23                                                & 63.015                  & 0.1421           \\ 
\hline
24                                                & 62.811                  & 0.1422           \\ 
\hline
25                                                & 62.629                  & 0.1419           \\ 
\hline
26                                                & 62.436                  & 0.1423           \\ 
\hline
27                                                & 62.424                  & 0.1416           \\ 
\hline
28                                                & 62.383                  & 0.1400           \\ 
\hline
29                                                & 62.144                  & 0.1383           \\ 
\hline
30                                                & 62.031                  & 0.1403           \\ 
\hline
31                                                & 62.002                  & 0.1382           \\ 
\hline
32                                                & 61.882                  & 0.1389           \\ 
\hline
33                                                & 61.649                  & 0.1370           \\ 
\hline
34                                                & 61.53                   & 0.1383           \\
\hline
\end{tabular}
\caption{\label{trainingdata}Top-5 accuracy and BLEU-4 score after each epoch during training. The row highlighted in yellow is the model that performed the best and what was used for validation and testing.}
\end{table}
% ------end of table------

The BLEU-4 score was incredibly low since the captions being generated by the model did not match any of the human-made caption. Since it is difficult to also trust such varied human-made captions as the gold-standard, a manual inspection of the generated captions was done. Below are some examples of what majority of the generated captions looked like:
% examples of captions
\begin{quote}
  ``white background''
\end{quote}
\begin{quote}
  ``a close up of a white background with a black background''
\end{quote}
\begin{quote}
  ``a close up of a black background with a white background with a white background''
\end{quote}
\begin{quote}
  ``a close up of a black background with black lettering''
\end{quote}
\begin{quote}
  ``the photo is cropped out of the frame''
\end{quote}

The captions seem to be recursive and only use words and phrases that refer to the image’s background, alternating from \emph{black} and \emph{white}. These captions are a result of the greedy search algorithm since a large amount of the human-made captions had these phrases at the end of the captions.
	
In evaluating the attention in \autoref{fig:stapler}, \autoref{fig:whitetext}, \autoref{fig:blackscreen}, there seems to be a pattern in where the model is looking while generating captions. The model begins by looking at the entire image, then attends to the middle of the image, typically in the top right and makes its way to the bottom left. The purpose of visualizing attention is not being achieved and these visualizations do not connect nor ground the captions it has generated for their respective images. 

% attention - stapler
\begin{figure}[ht]
  \includegraphics[width=\linewidth]{VizWiz_train_00013366.png}
  \caption{Attention visualized for captioning an image of a stapler in the wrong orientation (portrait instead of landscape)(VizWiz\_train\_00013366.png).}
  \label{fig:stapler}
\end{figure}
% ----end of figure----

% attention - whitetext
\begin{figure}[ht]
  \includegraphics[width=\linewidth]{VizWiz_train_00023423.png}
  \caption{Attention visualized for captioning an image of a white page with text (VizWiz\_train\_00023423.png).}
  \label{fig:whitetext}
\end{figure}
% ----end of figure----

% attention - black screen
\begin{figure}[ht]
  \includegraphics[width=\linewidth]{VizWiz_train_00000858.png}
  \caption{Attention visualized for captioning an image of a black screen with text in the wrong orientation (portrait instead of landscape)(VizWiz\_train\_00000858.png).}
  \label{fig:blackscreen}
\end{figure}
% ----end of figure----

\section{Discussion}
\label{sec:discussion}
\subsection{Domain-Specific Task}
\label{ssec:dstask}

\subsection{Dataset Limitations}
\label{ssec:datasetlimitations}

\subsection{Use of Attention}
\label{ssec:useofattention}


\section{Conclusion \& Future Work}
\label{sec:conc&fw}
\subsection{Future Work}
\label{ssec:futurework}

\subsubsection{Model Architecture}
\label{sssec:modarch}
\paragraph{Beam Search}
\paragraph{Visual Sentinel}

\subsubsection{Task-Specific Improvements}
\label{sssec:ds task}
\paragraph{Rule-based Template}
\paragraph{Optical Character Recognition}



\section*{Acknowledgments}

The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
Do not include this section when submitting your paper for review.

\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}



\end{document}
